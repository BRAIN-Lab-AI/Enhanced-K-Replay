# Knowledge-Enhanced-Image-Captioning


Below is a template for another sample project. Please follow this template.
# [Deep Learning Project Template] Enhanced K-Replay: Knowledge-Rich Image Captioning 

## Introduction
Enhanced K-Replay is an advanced deep learning model that addresses the challenge of generating accurate and knowledge-rich image captions. It enhances the K-Replay framework by integrating:
- **Beam search decoding** for generating diverse and accurate pseudo-captions,
- **Attention layers** to better focus on relevant image regions, 
- **Learning rate schedulers** to stabilize training.

These modifications help the model retain and express real-world knowledge, achieving notable improvements in both caption quality and concept recognition accuracy.

## Project Metadata
- **Author:** Reem AlJunaid  
- **Supervisor:** Dr. Muzammil Behzad  
- **Affiliations:** KFUPM and IAU

## Project Documents
- **Presentation:** [Project Presentation](/presentation.pptx)
- **Report:** [Project Report](/report.pdf)

## Reference Paper  
- Cheng et al., ‚ÄúBeyond generic: Enhancing image captioning with real-world knowledge using vision-language pre-training model‚Äù, *ACM MM 2023* [KnowCap paper](https://arxiv.org/abs/2308.01126)

## Datasets Used
<summary>üñºÔ∏è MS-COCO</summary>

- **Description**: A standard image captioning dataset.  
- **Split (Karpathy)**:  
  - Train: 113,287 images  
  - Validation: 5,000 images  
  - Test: 5,000 images  
- **Each image** is paired with five human-written captions.
</details>

<details>
<summary>üß† Replay CC12M Subset</summary>

- **Description**: A curated subset of over 20,000 samples extracted from CC12M.  
- **Filtering Criteria**: Image-text pairs that mention any of 122 predefined keywords.  
- **Use**: These samples are employed as replay exemplars during training.
</details>

<details>
<summary>üîç KnowCap Dataset</summary>

- **Description**: Enhances captioning with real-world knowledge.  
- **Total Pairs**: 1,424 image-caption pairs across 240 knowledge categories.  
- **Validation Set**: 424 samples  
- **Test Set**: 1,000 samples  
  - **Unseen Set**: 520 samples with 120 categories not in the predefined keyword list  
- **Use**: Evaluates the model‚Äôs generalization to new, unseen knowledge concepts.
</details>


## Terminologies
- **Image Captioning:** The task of generating natural language descriptions for images.
- **Vision-Language Pretraining (VLP):** Training models on large-scale image-text pairs to learn joint visual and textual representations.
- **Knowledge-rich Captions:** Captions that include contextual, domain-specific, or named-entity knowledge beyond generic descriptions.
- **K-Replay:** A continual learning framework that replays previously seen knowledge-rich samples to mitigate forgetting during fine-tuning.
- **Pseudo-Captions:** Captions generated by the model to simulate training targets, often used for weakly-supervised or replay training.
- **Beam Search:** A decoding strategy used to generate multiple diverse caption hypotheses, improving quality over greedy decoding.
- **Attention Mechanism:** A neural network component that helps the model focus on important regions of an image while generating captions.
- **Catastrophic Forgetting:** The tendency of neural networks to forget previously learned information when trained on new data.
- **Learning Rate Scheduler:** A strategy to adjust the learning rate during training to stabilize convergence and improve performance.


### Problem Statements
- **Problem 1:** Existing image captioning models tend to produce generic captions that miss contextual and real-world knowledge.  
- **Problem 2:** Vision-language pretraining (VLP) models struggle with zero-shot inference and often hallucinate knowledge.  
- **Problem 3:** Fine-tuning VLP models introduces a generic bias that limits knowledge expression.  
- **Problem 4:** The original K-Replay framework lacks stability and still leaves room for performance improvement.


## Proposed Enhancements
1. Replace greedy decoding with **beam search**.
2. Add **attention layers** to image encoders for better visual focus.
3. Use **cosine learning rate schedulers** for smoother convergence.

## Key Components
- `ofa_large_caption/`: Contains code for training with OFA-Large base.
- `scheduler.py`: Defines cosine learning rate scheduler.
- `attention_module.py`: Implements self-attention on image patches.
- `train.py`: Main training script with replay mechanism and loss functions.


### Key Components
- **`config.py`**: Contains configuration settings for training and evaluation.
- **`data/`**: JSON files for COCO, CC12M, and KnowCap datasets used during training and testing.
- **`data_load.py`**: Loads and preprocesses datasets for training and evaluation.
- **`test.py`**: Evaluation script for COCO dataset.
- **`test_knowcap.py`**: Evaluation script for KnowCap dataset.
- **`models/`**: Contains backbone models including OFA, BLIP, and GIT.
- **`train_multitask.py`**: Training script for the Enhanced K-Replay model.
- **`utils/`**: Includes utilities such as:
  - `beamsearch.py`: Beam search decoding logic.
  - `cc12m.py`: Replay sample filtering from CC12M.
  - `convert_ofa.py`: Checkpoint conversion for OFA.
  - `eval.py`: Caption generation and metric calculation.
  - `import_models.py`, `log.py`, `loss.py`, `optimizer_tools.py`: Misc. training and logging support.
  - `prepro_data.py`: Dataset construction and formatting.

## Training Algorithm

The Enhanced K-Replay model is trained using a mix of standard and knowledge-guided samples. The key steps of the training process are:
## Training Algorithm

The Enhanced K-Replay model is trained using a mix of standard and knowledge-guided samples. The key steps of the training process are:

### 1. Input:
- Mini-batch `b` sampled from:
  - Standard image-caption pairs: `Sc`
  - Knowledge-guided image-keyword pairs: `Sk`
- Model: `M_theta`
- Frozen reference model: `M_ref`
- Loss weights: `lambda_k`, `lambda_d`

### 2. Processing Each Sample:
- **If the sample is from `Sc`:**
  - Apply attention over image patches.
  - Encode image and caption using `M_theta` to get prediction `z`.
  - Compute text loss:  
    `L_txt = CrossEntropy(z, t)`

- **If the sample is from `Sk`:**
  - Generate pseudo-caption `t_hat` using beam search (beam size = 5).
  - Compute prediction `z = M_theta(i, t_hat)`.
  - Get hidden states `z_ref = M_ref(i, t_hat)`.
  - Compute keyword prediction loss:  
    `L_kpred = MSE(z, k)`
  - Compute distillation loss:  
    `L_distill = KL(z, z_ref)`

### 3. Total Loss:
L_total = L_txt + lambda_k * L_kpred + lambda_d * L_distill

### 4. Optimization:
- Update model parameters `theta` using AdamW optimizer with a learning rate scheduler.


## How to Run the Code

1. **Download the Images**
 * [COCO2014](https://github.com/ruotianluo/ImageCaptioning.pytorch/blob/master/data/README.md)
 * [KnowCap](https://drive.google.com/file/d/1DOk5WZZgHyO6tKT8A135hMgePid-akFq/view?usp=drive_link)
 * [Replay images selected from cc12m](https://drive.google.com/file/d/1tdVZ1rUpr5va-NwInMwBglRpSGOzUoMu/view?usp=drive_link)
 * [All] (https://drive.google.com/drive/folders/1N4OPMabt1mM48yI3IjyPd_aDEku-osSZ?usp=drive_link)

2. **Set Up Environment**
```bash
conda create -n knowcap python=3.8
conda activate knowcap
pip install -r requirements.txt
```

3. **Download Pretrained OFA-Large**
```bash
wget https://ofa-models.s3.amazonaws.com/ofa_large.pt
```

4. **Prepare Datasets**
Ensure datasets are downloaded and formatted into the `pycocoevalcap` format. Custom replay data can be generated via the `scripts/prepare_replay.py`.

5. **Train the Enhanced KnowCap Model**
```bash
python train.py   --model ofa_large   --scheduler cosine   --beam_size 5   --use_attention True   --replay_data path/to/cc12m_replay.json   --train_data path/to/coco_train.json
```

6. **Evaluate Model**
```bash
python evaluate.py --checkpoint path/to/model.pt --dataset knowcap
```

## Evaluation Metrics
- **CIDEr**
- **BLEU / ROUGE / METEOR**
- **Recognition Accuracy (for Knowledge concepts)**

## Results (Highlights)
| Technique           | CIDEr | Rec. Accuracy |
|---------------------|-------|----------------|
| OFA zero-shot       | 39.2  | 39.8%          |
| OFA + K-Replay      | 90.3  | 50.4%          |
| + Scheduler         | 92.6  | 54.2%          |
| + Beam Search       | 92.6  | **63.3%**      |
| + Attention         | 92.0  | 58.9%          |

## Acknowledgments
- Thanks to the KnowCap authors for open-sourcing their framework.
- Special appreciation to **Dr. Muzammil Behzad** for supervision and guidance.
- Gratitude to **KFUPM** and associated research staff for support.
